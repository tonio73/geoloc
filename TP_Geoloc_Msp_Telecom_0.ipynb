{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import geodesic\n",
    "from sklearn import model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import activations, datasets, layers, losses, metrics, models, backend, regularizers\n",
    "import tensorview as tv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_mess_train = pd.read_csv('mess_train_list.csv')\n",
    "\n",
    "# test set\n",
    "df_mess_test = pd.read_csv('mess_test_list.csv')\n",
    "\n",
    "# position associated to train set\n",
    "pos_train = pd.read_csv('pos_train_list.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess_train['did'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mess_train.shape)\n",
    "df_mess_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine all Base stations that received at least 1 message\n",
    "trainBs  = np.unique(df_mess_train['bsid'])\n",
    "testBs   = np.unique(df_mess_test['bsid'])\n",
    "listOfBs = np.union1d(trainBs, testBs) \n",
    "testOnlyBs = np.lib.arraysetops.setdiff1d(testBs, trainBs)\n",
    "\n",
    "print(f\"Number of stations: %d, test only %d\" % (len(listOfBs), len(testOnlyBs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess_train['did'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passage en UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def latlon_to_xy(lat, lon):\n",
    "    \"\"\"Conversion lat/lon en UTM\"\"\"\n",
    "    x, y, utm_zone, utm_letter = utm.from_latlon(lat, lon)\n",
    "    return x, y, utm_zone, utm_letter\n",
    "\n",
    "\n",
    "def xy_to_latlon(x, y, utm_zone, utm_letter):\n",
    "    \"\"\"Conversion UTM en lat/lon\"\"\"\n",
    "    lat, lon = utm.to_latlon(x, y, utm_zone, utm_letter)\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "pos_train[['x', 'y', 'utm_zone', 'utm_letter']] = pos_train.apply(lambda row: pd.Series(latlon_to_xy(row['lat'], row['lng'])),axis=1)\n",
    "df_mess_train[['bs_x', 'bs_y', 'bs_utm_zone', 'bs_utm_letter']] = df_mess_train.apply(lambda row: pd.Series(latlon_to_xy(row['bs_lat'], row['bs_lng'])),axis=1)\n",
    "df_mess_test[['bs_x', 'bs_y', 'bs_utm_zone', 'bs_utm_letter']] = df_mess_test.apply(lambda row: pd.Series(latlon_to_xy(row['bs_lat'], row['bs_lng'])),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering on outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombres de messages du jeu d'apprentissage: {len(df_mess_train.messid.unique())}\");\n",
    "print(f\"Nombres de messages du jeu d'apprentissage sans les stations au dessus du 60eme parallèle : {len(df_mess_train[df_mess_train.bs_lat<60].messid.unique())}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombres de messages du jeu de: {len(df_mess_test.messid.unique())}\");\n",
    "print(f\"Nombres de messages du jeu de test sans les stations au dessus du 60eme parallèle : {len(df_mess_test[df_mess_test.bs_lat<60].messid.unique())}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse\n",
    "On a 150 messages du jeu d'apprentissage et 95 du jeu de test qui refère uniquement les stations au dessus du 60eme parallèle.\n",
    "Ces stations sont très éloignés des autres données et semblent donc être des données anomaliques.\n",
    "En analysant les coordonnées des labels pour ces messages, ils s'avèrent que les devices sont tous situé dans la zone des autres stations.\n",
    "Nous prenons donc le parti de modifier les coordonnées de ces stations au barycentre des différentes positions des devices qui ont communiqué uniquement avec ces stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~df_mess_train[df_mess_train.bs_lat > 60][\"messid\"].isin(df_mess_train[df_mess_train.bs_lat < 60][\"messid\"])\n",
    "messid_far = df_mess_train[df_mess_train.bs_lat > 60][\"messid\"][mask].unique()\n",
    "df_contat = pd.concat([df_mess_train,pos_train], axis=1)\n",
    "zone_19_stations = df_contat[df_contat[\"messid\"].isin(messid_far)]\n",
    "zone_19_coords = zone_19_stations.mean(axis=0)[[\"lat\", \"lng\", \"x\",\"y\",\"utm_zone\"]]\n",
    "\n",
    "mask_19 = df_mess_train[\"bs_utm_zone\"]!=19\n",
    "df_mess_train.bs_lat = df_mess_train.bs_lat.where(mask_19, other=zone_19_coords.lat)\n",
    "df_mess_train.bs_lng = df_mess_train.bs_lng.where(mask_19, other=zone_19_coords.lng)\n",
    "df_mess_train.bs_x = df_mess_train.bs_x.where(mask_19, other=zone_19_coords.x)\n",
    "df_mess_train.bs_y = df_mess_train.bs_y.where(mask_19, other=zone_19_coords.y)\n",
    "df_mess_train.bs_utm_zone = df_mess_train.bs_utm_zone.where(mask_19, other=zone_19_coords.utm_zone)\n",
    "df_mess_train.bs_utm_letter = df_mess_train.bs_utm_letter.where(mask_19, other=\"S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_19 = df_mess_test[\"bs_utm_zone\"]!=19\n",
    "df_mess_test.bs_lat = df_mess_test.bs_lat.where(mask_19, other=zone_19_coords.lat)\n",
    "df_mess_test.bs_lng = df_mess_test.bs_lng.where(mask_19, other=zone_19_coords.lng)\n",
    "df_mess_test.bs_x = df_mess_test.bs_x.where(mask_19, other=zone_19_coords.x)\n",
    "df_mess_test.bs_y = df_mess_test.bs_y.where(mask_19, other=zone_19_coords.y)\n",
    "df_mess_test.bs_utm_zone = df_mess_test.bs_utm_zone.where(mask_19, other=zone_19_coords.utm_zone)\n",
    "df_mess_test.bs_utm_letter = df_mess_test.bs_utm_letter.where(mask_19, other=\"S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la matrice des featues\n",
    "Pour cette matrice, nous n'utiliserons qu'une partie des données et nous ferons une moyenne geometrique sur les coordonnées des devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_mat_const(df, listOfBs, keepMax=5):\n",
    "    \"\"\" Feature Matrix construction \"\"\"\n",
    "    \n",
    "    aggCols = ['did', 'pivot_lat', 'pivot_lng']\n",
    "    for i in range(keepMax):\n",
    "        bsCols =['bs%d_deltalat' % i, 'bs%d_deltalng' % i, 'bs%d_rssi' % i] #, 'bs%d_nseq' % i 'bs%d_active' % i, \n",
    "        aggCols = aggCols + bsCols\n",
    "        \n",
    "    def aggregateBaseStations(groupBy):\n",
    "        \"\"\" From a RSSI sorted DataFrameGroupBy\n",
    "            create a dataframe with the 3 best BS \n",
    "        \"\"\"\n",
    "        \n",
    "        did = groupBy['did'][0]\n",
    "        bsSet = groupBy.iloc[:keepMax]\n",
    "            \n",
    "        # Barycentre par moyenne geometrique\n",
    "        w_geom = np.exp(bsSet['rssi']) / np.sum(np.exp(bsSet['rssi']))\n",
    "        \n",
    "        x_geom = np.exp(np.sum(w_geom * np.log(bsSet['bs_x'])) / np.sum(w_geom))\n",
    "        y_geom = np.exp(np.sum(w_geom * np.log(bsSet['bs_y'])) / np.sum(w_geom))\n",
    "        \n",
    "        # Barycentre par moyenne arithmétique\n",
    "        w_arm = bsSet['rssi'] / np.sum(bsSet['rssi'])\n",
    "        \n",
    "        x_arm = np.average(bsSet['bs_x'], weights=w_arm)\n",
    "        y_arm = np.average(bsSet['bs_y'], weights=w_arm)\n",
    "        \n",
    "        bss = []\n",
    "        for i in range(keepMax):\n",
    "            if len(bsSet) > i:\n",
    "                b = bsSet.iloc[i]\n",
    "                dx = b['bs_x'] - x_geom\n",
    "                dy = b['bs_y'] - y_geom\n",
    "                bss.append([dy, dx, b['rssi']])\n",
    "            else:\n",
    "                bss.append([0, 0, -1e3])\n",
    "        return pd.DataFrame(np.concatenate([[did, y_geom, x_geom], np.array(bss).ravel()]).reshape(1, -1), \n",
    "                            columns=aggCols)\n",
    "            \n",
    "    \n",
    "    # Keep at max keepMax base-stations per message\n",
    "    df = df.groupby('messid'). \\\n",
    "        apply(lambda x: x.sort_values(['rssi'], ascending=False)). \\\n",
    "        reset_index(drop=True).groupby('messid').apply(aggregateBaseStations)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_const(df_mess_train, pos_train):\n",
    "    \"\"\" Ground truth construction \"\"\"\n",
    "    \n",
    "    df = pd.concat([df_mess_train[['messid']], pos_train], axis=1)\n",
    "    df_mean = df.groupby('messid').mean()\n",
    "\n",
    "    return df_mean['y'], df_mean['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = feat_mat_const(df_mess_train, listOfBs, 5)\n",
    "print(df_feat.shape)\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_y, ground_truth_x = ground_truth_const(df_mess_train, pos_train)\n",
    "ground_truth_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_red = df_feat.drop('did', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vincenty_vec(vec_coord):\n",
    "    \"\"\" Now using geodesic distance instead of Vincenty \"\"\"\n",
    "    vin_vec_dist = np.zeros(vec_coord.shape[0])\n",
    "    if vec_coord.shape[1] != 4:\n",
    "        print('ERROR: Bad number of columns (shall be = 4)')\n",
    "    else:\n",
    "        vin_vec_dist = [geodesic(v[0:2], v[2:]).meters for v in vec_coord]\n",
    "\n",
    "    return vin_vec_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_geoloc(y_train_lat , y_train_lng, y_pred_lat, y_pred_lng):\n",
    "    \"\"\" Evaluate distance error for each predicted point \"\"\"\n",
    "    vec_coord = np.array([y_train_lat , y_train_lng, y_pred_lat, y_pred_lng])\n",
    "    err_vec = vincenty_vec(np.transpose(vec_coord))\n",
    "    \n",
    "    return err_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotError(err_vec):\n",
    "    \"\"\" Plot error cumulative distribution and the 80 quantile \"\"\"\n",
    "    \n",
    "    err80 = np.percentile(err_vec, 80)\n",
    "    \n",
    "    print(f\"error @ 80% = {err80:.1f} m\")\n",
    "    \n",
    "    values, base = np.histogram(err_vec, bins=50000)\n",
    "    cumulative = np.cumsum(values) \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(base[:-1]/1000, cumulative / np.float(np.sum(values))  * 100.0,\n",
    "             label=\"Opt LLR\", c='blue')\n",
    "\n",
    "    # plot error @ 80%\n",
    "    plt.axvline(x=err80/1000., ymin=0, ymax=100,\n",
    "                linestyle='dashed', color='red')\n",
    "\n",
    "    plt.xlabel('Distance Error (km)')\n",
    "    plt.ylabel('Cum proba (%)')\n",
    "    plt.axis([0, 30, 0, 100]) \n",
    "\n",
    "    plt.title('Error Cumulative Probability')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = model_selection.train_test_split(df_feat_red.values, \n",
    "                                                                np.c_[ground_truth_y.values, ground_truth_x.values], \n",
    "                                                                test_size=0.1)\n",
    "# Normalize data to get proper network optimization\n",
    "scaleX = preprocessing.StandardScaler()\n",
    "scaleX.fit(xtrain)\n",
    "xtrain = scaleX.transform(xtrain)\n",
    "xtest = scaleX.transform(xtest)\n",
    "\n",
    "scaleY = preprocessing.StandardScaler()\n",
    "scaleY.fit(ytrain)\n",
    "ytrain = scaleY.transform(ytrain)\n",
    "# NO ytest = scaleY.transform(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = models.Sequential([\n",
    "    layers.Dense(128, name='dense_1', activation=activations.relu, input_shape=[df_feat_red.shape[1]]),\n",
    "    layers.Dropout(0.01),\n",
    "    layers.Dense(32, name='dense_2', activation=activations.relu),\n",
    "   #  layers.Dropout(0.01),\n",
    "    layers.Dense(2, name='dense_3', activation=activations.linear),\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "          loss=losses.MeanSquaredError())\n",
    "    \n",
    "model1.summary()\n",
    "\n",
    "metricNames = ['Loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 96\n",
    "batchSize = 64\n",
    "\n",
    "tvPlot = tv.train.PlotMetricsOnEpoch(metrics_name=metricNames,\n",
    "                                      cell_size=(6,4), columns=2, iter_num=nEpochs, wait_num=1)\n",
    "\n",
    "history1 = model1.fit(xtrain, ytrain,\n",
    "            epochs=nEpochs, batch_size=batchSize, \n",
    "            validation_split=0.1, \n",
    "            verbose=0,\n",
    "            callbacks=[tvPlot]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = model1.get_weights()\n",
    "plt.hist(weights1[0].ravel(), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yEst = model1.predict(xtest)\n",
    "yEst = scaleY.inverse_transform(yEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(ytest[:,0], yEst[:,0]).numpy(), \\\n",
    "metrics.mean_squared_error(ytest[:,1], yEst[:,1]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dnnErr_vec = eval_geoloc(ytest[:,0], ytest[:,1], yEst[:,0].reshape(-1), yEst[:,1].reshape(-1))\n",
    "#plotError(dnnErr_vec)\n",
    "\n",
    "# The coordinates are in true X,Y for distance computation\n",
    "delta_y = yEst[:,0] - ytest[:,0]\n",
    "delta_x = yEst[:,1] - ytest[:,1]\n",
    "\n",
    "\n",
    "dnnErr_vec = np.sqrt(delta_x**2 + delta_y**2)\n",
    "\n",
    "plotError(dnnErr_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN with leave out device K fold validation\n",
    "\n",
    "Utilisation de GroupKFold de Scikit Learn, génère K fold en évitant que les éléments d'un groupe se trouvent dans le train et la valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkl1o = model_selection.GroupKFold(24) #model_selection.LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleX = preprocessing.StandardScaler()\n",
    "scaleX.fit(df_feat_red)\n",
    "df_feat_scaled = scaleX.transform(df_feat_red)\n",
    "\n",
    "y = np.c_[ground_truth_y.values, ground_truth_x.values]\n",
    "scaleY = preprocessing.StandardScaler()\n",
    "scaleY.fit(y)\n",
    "yscaled = scaleY.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "errors80Dnn = []\n",
    "nEpochs = 96\n",
    "for train, val in gkl1o.split(df_feat, groups=df_feat['did']):\n",
    "    \n",
    "    i+=1\n",
    "    if i % 4 == 0: print('Split #', i)  \n",
    "    \n",
    "    xtrain = df_feat_scaled[train]\n",
    "    xval = df_feat_scaled[val]\n",
    "    ytrain = yscaled[train]\n",
    "    yval = yscaled[val]\n",
    "    \n",
    "    model1.reset_states()\n",
    "    model1.fit(xtrain, ytrain,\n",
    "            epochs=nEpochs, batch_size=batchSize, \n",
    "            validation_data=(xval, yval),\n",
    "            verbose=0)\n",
    "    yEst = model1.predict(xval)\n",
    "    yEst = scaleY.inverse_transform(yEst)\n",
    "    \n",
    "    delta_y = yEst[:,0] - ground_truth_y[val]\n",
    "    delta_x = yEst[:,1] - ground_truth_x[val]\n",
    "    \n",
    "    dnnErr_vec = np.sqrt(delta_x**2 + delta_y**2)\n",
    "    \n",
    "    #dnnErr_vec = eval_geoloc(ground_truth_lat[val], ground_truth_lng[val], yEst[:,0], yEst[:,1])\n",
    "    err80 = np.percentile(dnnErr_vec, 80)\n",
    "    #print('Err @ 80%%, %.1fm' % (err80))\n",
    "    errors80Dnn.append(err80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with Leave out device K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 250,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "i = 0\n",
    "didLgbm = []\n",
    "errors80Lgbm = []\n",
    "for train, val in gkl1o.split(df_feat, groups=df_feat['did']):\n",
    "    i+=1\n",
    "    if i % 4 == 0:\n",
    "        print('Split #', i)\n",
    "    \n",
    "    xtrain = df_feat_red.values[train]\n",
    "    xval = df_feat_red.values[val]\n",
    "    \n",
    "    lat_train = lightgbm.Dataset(xtrain, ground_truth_y[train])\n",
    "    lat_valid = lightgbm.Dataset(xval, ground_truth_y[val])\n",
    "    model_lat = lightgbm.train(params,\n",
    "                           lat_train,\n",
    "                           valid_sets=lat_valid,\n",
    "                           num_boost_round=5000,\n",
    "                           early_stopping_rounds=250,verbose_eval=False) \n",
    "\n",
    "    lng_train = lightgbm.Dataset(xtrain, ground_truth_x[train])\n",
    "    lng_valid = lightgbm.Dataset(xval, ground_truth_x[val])\n",
    "    model_lng = lightgbm.train(params,\n",
    "                           lng_train,\n",
    "                           valid_sets=lng_valid,\n",
    "                           num_boost_round=5000,\n",
    "                           early_stopping_rounds=250,verbose_eval=False) \n",
    "\n",
    "    # Evaluate\n",
    "    lat_pred = model_lat.predict(xval)\n",
    "    lng_pred = model_lng.predict(xval)\n",
    "    \n",
    "    delta_y = lat_pred - ground_truth_y[val]\n",
    "    delta_x = lng_pred - ground_truth_x[val]\n",
    "    \n",
    "    lgbmErr_vec = np.sqrt(delta_x**2 + delta_y**2)\n",
    "    #lgbmErr_vec = eval_geoloc(ground_truth_y[val], ground_truth_x[val], lat_pred, lng_pred)\n",
    "    err80 = np.percentile(lgbmErr_vec, 80)\n",
    "    errors80Lgbm.append(err80) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lat_pred = model_lat.predict(xtest)\n",
    "#lng_pred = model_lng.predict(xtest)\n",
    "#\n",
    "#lgbmErr_vec = eval_geoloc(ytest[:,0], ytest[:,1], lat_pred, lng_pred)\n",
    "#plotError(lgbmErr_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot leave 1 device out performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorMeanOverDevicesDnn = np.array(errors80Dnn).mean()\n",
    "errorMeanOverDevicesLgbm = np.array(errors80Lgbm).mean()\n",
    "print('Mean error over devices @ 80%%, DNN : %.1fm, LGBM : %.1fm' % \\\n",
    "          (errorMeanOverDevicesDnn, errorMeanOverDevicesLgbm))\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "#ax.hist(errors80Dnn,  bins=5, alpha=0.7, label='DNN')\n",
    "ax.hist(errors80Lgbm, bins=10, alpha=0.7, label='LGBM')\n",
    "ax.set_title('Histogram of error on the 1 device out')\n",
    "ax.set_xlabel('Error@ 80%[m]')\n",
    "#ax.axvline(x=errorMeanOverDevicesDnn,  color='blue', linestyle='--')\n",
    "ax.axvline(x=errorMeanOverDevicesLgbm, color='orange', linestyle='--')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study outliers on the error 80\n",
    "\n",
    "Device ID=476835 a des performances très mauvaises (erreur @ 80% > 100km).\n",
    "\n",
    "Cette section détermine les problèmes des messages de ce device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier1Did = 476835\n",
    "outlier1Index = df_mess_train['did'] == outlier1Did\n",
    "outlier1X = df_mess_train[outlier1Index]\n",
    "outlier1X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier1MessId = df_mess_train[outlier1Index].reset_index()['messid'].unique()\n",
    "outlier1MessId.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_rank1OutlierIndex = df_feat['did']== outlier1Did\n",
    "df_feat[df_feat_rank1OutlierIndex].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier1y = y[df_feat_rank1OutlierIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation du dernier estimateur LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharey=True)\n",
    "error_lat = model_lat.predict(df_feat_red.loc[outlier1MessId]) - outlier1y[:,0]\n",
    "axes[0].plot(error_lat)\n",
    "axes[0].set_title('Rank 1 outlier latitude error')\n",
    "axes[0].set_xlabel('message')\n",
    "axes[0].set_ylabel('lat error [deg]')\n",
    "axes[0].grid()\n",
    "error_lng = model_lng.predict(df_feat_red.loc[outlier1MessId]) - outlier1y[:,1]\n",
    "axes[1].plot(error_lng)\n",
    "axes[1].set_title('Rank 1 outlier longitude error')\n",
    "axes[1].set_xlabel('message')\n",
    "axes[1].set_ylabel('lng error [deg]')\n",
    "axes[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_red.loc[outlier1MessId][50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 17 derniers messages n'ont qu'une seule BS et le device s'éloigne de la BS.\n",
    "\n",
    "Ils sont affichés en bleu sur la carte suivante, au sud de Denver. Les BS, de tous les messages de ce device, sont en rouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    location=[39.8, -105.0],\n",
    "    zoom_start=6,\n",
    "    tiles='Stamen Terrain'\n",
    ")\n",
    "\n",
    "for r in outlier1X.iterrows():\n",
    "    row = r[1]\n",
    "    folium.Circle(\n",
    "        radius=max(5, 200+row['rssi']),\n",
    "        location=[row['bs_lat'], row['bs_lng']],\n",
    "        #popup='The Waterfront',\n",
    "        color='crimson',\n",
    "        fill=False,\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "for row in outlier1y[52:]:\n",
    "    #row = r[1]\n",
    "    lat, lon = xy_to_latlon(row[1], row[0], 13,'S')\n",
    "    folium.Circle(\n",
    "        radius=10,\n",
    "        location=[lat, lon],\n",
    "        #popup='The Waterfront',\n",
    "        color='blue',\n",
    "        fill=False,\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mess_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_test = feat_mat_const(df_mess_test, listOfBs)\n",
    "df_feat.shape, df_feat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_lat, y_pred_test_lng, reg = regressor_and_predict(df_feat, ground_truth_lat, \n",
    "                                                    ground_truth_lng, df_feat_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = pd.DataFrame(np.array([y_pred_test_lat, y_pred_test_lng]).T, columns = ['lat', 'lng'])\n",
    "test_res = pd.concat([df_mess_test['messid'], test_res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res.to_csv('pred_pos_test_list.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
